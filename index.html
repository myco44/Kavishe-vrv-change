<!doctype html>
<html lang="sw">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>Kavishe Voice Lab ‚Äî Improved (Record, Upload, Presets, TTS)</title>
<style>
  :root{--bg:#061021;--card:#082035;--accent:#4da3ff;--muted:#9fb3d8}
  body{margin:0;font-family:system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Arial;background:linear-gradient(180deg,var(--bg),#04101a);color:#eaf3ff}
  .wrap{max-width:1100px;margin:18px auto;padding:12px;display:grid;grid-template-columns:1fr;gap:12px}
  h1{margin:0 0 6px;font-size:20px}
  .card{background:var(--card);border-radius:10px;padding:12px;border:1px solid rgba(255,255,255,0.03)}
  .row{display:flex;gap:8px;flex-wrap:wrap;align-items:center}
  button,select,input[type="range"],input[type="file"],textarea{background:#06293f;border:1px solid rgba(255,255,255,0.04);padding:8px;border-radius:8px;color:inherit}
  button.primary{background:var(--accent);color:#012030}
  .small{font-size:13px;color:var(--muted)}
  #waveCanvas{width:100%;height:80px;background:#00121a;border-radius:6px}
  #videoPreview{width:100%;max-height:240px;background:#000;border-radius:6px}
  .presetList{display:flex;gap:8px;flex-wrap:wrap}
  .presetBtn{padding:6px 10px;border-radius:8px;background:#072a3a;border:1px solid rgba(255,255,255,0.03);cursor:pointer}
  .danger{background:#8b2233}
  .note{font-size:12px;color:#cdd9ff}
</style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Kavishe Voice Lab ‚Äî Improved</h1>
      <div class="small">Record, upload, apply presets, Text‚ÜíSpeech (lahaja), play/stop, save processed WAV. Presets ni approximations. Usitumie bila idhini.</div>
    </div>

    <!-- Recording + Upload -->
    <div class="card">
      <div class="row">
        <button id="startRec" class="primary">üéôÔ∏è Start Recording</button>
        <button id="stopRec" disabled>‚èπÔ∏è Stop Recording</button>
        <button id="playRaw" disabled>‚ñ∂ Raw</button>
        <button id="playFx" disabled>‚ñ∂ Preset Play</button>
        <button id="stopPlay" class="danger" disabled>‚ñ† Stop Play</button>
        <button id="saveFx" disabled>üíæ Save Processed WAV</button>
        <label class="small" style="margin-left:auto">Pitch: <input id="pitch" type="range" min="0.6" max="1.8" step="0.01" value="1.0"/><span id="pitchVal">1.00x</span></label>
      </div>

      <div style="margin-top:10px" class="row">
        <label class="small">Formant (sim): <input id="formant" type="range" min="-4" max="4" step="0.1" value="0"/><span id="formVal">0.0</span></label>
        <label class="small">Reverb <input id="fxReverb" type="checkbox"/></label>
        <label class="small">Distort <input id="fxDist" type="checkbox"/></label>
        <label class="small">Robot <input id="fxRobot" type="checkbox"/></label>
      </div>

      <div style="margin-top:10px" class="row">
        <input id="upload" type="file" accept="audio/*"/>
        <button id="useUpload" disabled>Use Uploaded</button>
        <div class="small" style="margin-left:auto">Recorded file kept in memory ‚Äî click Save to download.</div>
      </div>

      <div style="margin-top:10px">
        <canvas id="waveCanvas"></canvas>
      </div>
    </div>

    <!-- Presets -->
    <div class="card">
      <h3>Presets ‚Äî chagua moja</h3>
      <div class="presetList" id="presetList">
        <!-- many presets -->
      </div>
      <div style="margin-top:8px" class="row">
        <button id="applyPreset" class="primary">Apply Preset (for Play)</button>
        <button id="storePreset">Save as Custom</button>
      </div>
      <div class="note" style="margin-top:8px">Presets ni set ya pitch+formant+fx. TTS lahaja zinapatikana chini.</div>
    </div>

    <!-- Text -> Speech -->
    <div class="card">
      <h3>Text ‚Üí Speech (andika hapa) ‚Äî chagua lahaja</h3>
      <textarea id="ttsText" placeholder="Andika hapa... (Kiswahili)"></textarea>
      <div class="row" style="margin-top:8px">
        <select id="ttsLang">
          <option value="sw-TZ">Kiswahili (Tanzania) - sw-TZ</option>
          <option value="sw-KE">Kiswahili (Kenya) - sw-KE</option>
          <option value="en-US">English (US)</option>
          <option value="en-GB">English (UK)</option>
          <option value="pt-BR">Portugu√™s (BR)</option>
          <option value="ar-SA">Arabic (SA)</option>
        </select>
        <button id="speak">üîà Speak</button>
        <button id="speakWithPreset" disabled>üîà Speak + Apply Preset (approx)</button>
        <label class="small" style="margin-left:auto">Rate<input id="ttsRate" type="range" min="0.6" max="1.6" step="0.05" value="1.0"/><span id="ttsRateVal">1.00</span></label>
      </div>
      <div class="note" style="margin-top:8px">Note: TTS voices and quality depend on browser/device. Applying FX to TTS is approximate (we use speechSynthesis for direct speak; for FX we attempt to capture via WebAudio where supported).</div>
    </div>

    <!-- Video cartoon is omitted here for brevity; keep previous if needed -->

    <div class="card">
      <div class="small">Status / Log:</div>
      <textarea id="log" readonly style="height:120px"></textarea>
    </div>
  </div>

<script>
/* Improved Voice Lab: record/upload/play with FX, typed-TTS, presets.
   Limitations explained inline.
*/

// Basic logger
const logEl = document.getElementById('log');
function log(msg){ logEl.value = new Date().toLocaleTimeString() + ' ‚Ä¢ ' + msg + '\\n' + logEl.value; }

// UI references
const startRec = document.getElementById('startRec');
const stopRec = document.getElementById('stopRec');
const playRaw = document.getElementById('playRaw');
const playFx = document.getElementById('playFx');
const stopPlay = document.getElementById('stopPlay');
const saveFx = document.getElementById('saveFx');
const upload = document.getElementById('upload');
const useUpload = document.getElementById('useUpload');
const pitch = document.getElementById('pitch');
const pitchVal = document.getElementById('pitchVal');
const formant = document.getElementById('formant');
const formVal = document.getElementById('formVal');
const fxReverb = document.getElementById('fxReverb');
const fxDist = document.getElementById('fxDist');
const fxRobot = document.getElementById('fxRobot');
const waveCanvas = document.getElementById('waveCanvas');

const applyPresetBtn = document.getElementById('applyPreset');
const presetListDiv = document.getElementById('presetList');
const storePresetBtn = document.getElementById('storePreset');

const ttsText = document.getElementById('ttsText');
const ttsLang = document.getElementById('ttsLang');
const speakBtn = document.getElementById('speak');
const speakWithPresetBtn = document.getElementById('speakWithPreset');
const ttsRate = document.getElementById('ttsRate');
const ttsRateVal = document.getElementById('ttsRateVal');

let mediaStream = null;      // mic stream used for recording
let mediaRecorder = null;
let recordedChunks = [];
let audioContext = null;
let rawAudioBuffer = null;   // AudioBuffer of recorded or uploaded audio
let currentSource = null;    // AudioBufferSourceNode used for playback (so we can stop)
let currentConvolver = null;
let currentOsc = null;       // for robot
let uploadFileBuffer = null; // arrayBuffer of uploaded file

// ---------- helpers ----------
function ensureAudioContext(){
  if(!audioContext) audioContext = new (window.AudioContext||window.webkitAudioContext)();
  return audioContext;
}

// ---------- Recording ----------
startRec.onclick = async () => {
  try {
    if(!mediaStream) mediaStream = await navigator.mediaDevices.getUserMedia({audio:true});
    recordedChunks = [];
    mediaRecorder = new MediaRecorder(mediaStream, {mimeType:'audio/webm;codecs=opus'});
    mediaRecorder.ondataavailable = e => { if(e.data && e.data.size) recordedChunks.push(e.data); };
    mediaRecorder.onstop = async () => {
      const blob = new Blob(recordedChunks, {type: 'audio/webm'});
      const arrayBuffer = await blob.arrayBuffer();
      const ctx = ensureAudioContext();
      rawAudioBuffer = await ctx.decodeAudioData(arrayBuffer.slice(0)); // decode
      log('Recording saved to buffer. Duration: ' + rawAudioBuffer.duration.toFixed(2) + 's');
      playRaw.disabled = false; playFx.disabled = false; saveFx.disabled = false; speakWithPresetBtn.disabled=false;
      drawWave(rawAudioBuffer);
    };
    mediaRecorder.start();
    startRec.disabled = true; stopRec.disabled = false;
    log('Recording started...');
  } catch (e) {
    alert('Mic permission required: ' + e.message);
    log('Recording error: ' + e.message);
  }
};

stopRec.onclick = () => {
  if(mediaRecorder && mediaRecorder.state === 'recording') mediaRecorder.stop();
  stopRec.disabled = true; startRec.disabled = false;
  log('Recording stopped.');
};

// ---------- Draw waveform ----------
function drawWave(buffer){
  const canvas = waveCanvas;
  const ctx = canvas.getContext('2d');
  const dpr = devicePixelRatio || 1;
  const w = canvas.clientWidth * dpr, h = canvas.clientHeight * dpr;
  canvas.width = w; canvas.height = h;
  ctx.fillStyle = '#00121a'; ctx.fillRect(0,0,w,h);
  const data = buffer.getChannelData(0);
  const step = Math.ceil(data.length / w);
  ctx.lineWidth = 1 * dpr;
  ctx.strokeStyle = '#58d0ff';
  ctx.beginPath();
  for(let i=0;i<w;i++){
    let sum=0;
    for(let j=0;j<step;j++){
      sum += Math.abs(data[i*step + j] || 0);
    }
    const v = sum/step;
    const y = (1 - v) * (h/2);
    if(i===0) ctx.moveTo(i,y); else ctx.lineTo(i,y);
  }
  ctx.stroke();
}

// ---------- Playback (raw) ----------
playRaw.onclick = async () => {
  if(!rawAudioBuffer){ alert('Hakuna sauti'); return; }
  const ctx = ensureAudioContext();
  stopCurrentPlayback();
  const src = ctx.createBufferSource(); src.buffer = rawAudioBuffer;
  src.connect(ctx.destination);
  src.start();
  currentSource = src;
  stopPlay.disabled = false;
  log('Playing raw audio');
  src.onended = ()=>{ stopCurrentPlayback(); log('Raw playback ended'); };
};

function stopCurrentPlayback(){
  try{
    if(currentSource){ currentSource.stop(); currentSource.disconnect(); currentSource = null; }
    if(currentOsc){ currentOsc.stop?.(); currentOsc = null; }
    if(currentConvolver){ currentConvolver.disconnect(); currentConvolver = null; }
  }catch(e){ /* ignore */ }
  stopPlay.disabled = true;
}

// ---------- Apply FX & playback (preset play) ----------
function makeDistortion(ctx, amount=200){
  const n = 44100;
  const curve = new Float32Array(n);
  const k = typeof amount === 'number' ? amount : 50;
  for(let i=0;i<n;i++){
    const x = i * 2 / n - 1;
    curve[i] = (3 + k) * x * 20 * Math.PI/180 / (Math.PI + k * Math.abs(x));
  }
  const shaper = ctx.createWaveShaper();
  shaper.curve = curve;
  shaper.oversample = '4x';
  return shaper;
}

// simple impulse for reverb
function makeImpulse(ctx, duration=1.5, decay=2){
  const sr = ctx.sampleRate;
  const len = Math.floor(sr * duration);
  const buf = ctx.createBuffer(2, len, sr);
  for(let c=0;c<2;c++){
    const cd = buf.getChannelData(c);
    for(let i=0;i<len;i++){
      cd[i] = (Math.random()*2-1) * Math.pow(1 - i/len, decay);
    }
  }
  return buf;
}

async function playWithPreset(buffer, preset){
  if(!buffer) { alert('No audio to play'); return; }
  const ctx = ensureAudioContext();
  stopCurrentPlayback();
  const src = ctx.createBufferSource();
  src.buffer = buffer;
  // Pitch: playbackRate (simple) ‚Äî note: changes speed
  src.playbackRate.value = preset.pitch || 1.0;

  // Filters to approximate formant shift
  const low = ctx.createBiquadFilter(); low.type='lowshelf'; low.frequency.value = 300; low.gain.value = (preset.formant||0)*-2;
  const high = ctx.createBiquadFilter(); high.type='highshelf'; high.frequency.value=3000; high.gain.value=(preset.formant||0)*2;

  // bandpass for "mzee/telephone"
  const band = ctx.createBiquadFilter(); band.type='bandpass'; band.frequency.value = 800; band.Q.value = preset.bandQ||1;

  src.connect(low); low.connect(high); high.connect(band);

  let last = band;

  // distortion
  if(preset.distort){
    const sh = makeDistortion(ctx, 400);
    last.connect(sh); last = sh;
  }

  // reverb
  if(preset.reverb){
    const conv = ctx.createConvolver(); conv.buffer = makeImpulse(ctx, 1.5, 2.0);
    last.connect(conv); last = conv;
    currentConvolver = conv;
  }

  // robot (ring-mod)
  if(preset.robot){
    const osc = ctx.createOscillator(); osc.type='square'; osc.frequency.value = preset.robotFreq || 30;
    const gain = ctx.createGain(); gain.gain.value = 0.5;
    osc.connect(gain);
    // Multiply signal amplitude by LFO -> approximate ring-mod via gain node modulation
    last.connect(ctx.destination); // also route directly for fallback (we'll use a separate approach)
    // Simpler approach: route last through a GainNode whose gain is modulated by oscillator
    const modGain = ctx.createGain(); modGain.gain.value = 1.0;
    last.connect(modGain);
    gain.connect(modGain.gain);
    modGain.connect(ctx.destination);
    osc.start();
    currentOsc = osc;
    src.start();
    currentSource = src;
    stopPlay.disabled = false;
    log('Playing with robot preset (approx)');
    src.onended = ()=> { stopCurrentPlayback(); log('Preset play ended'); };
    return;
  }

  last.connect(ctx.destination);
  src.start();
  currentSource = src;
  stopPlay.disabled = false;
  log('Playing with preset: ' + (preset.name || 'custom'));
  src.onended = ()=>{ stopCurrentPlayback(); log('Preset playback ended'); };
}

// ---------- Preset management ----------
const presets = [
  {name:'neutral', pitch:1.0, formant:0, reverb:false, distort:false, robot:false, bandQ:1},
  {name:'mtoto_kike', pitch:1.6, formant:2.4, reverb:false, distort:false, robot:false, bandQ:1},
  {name:'mtoto_mvulana', pitch:1.45, formant:1.6, reverb:false, distort:false, robot:false, bandQ:1},
  {name:'mwanaume', pitch:0.92, formant:-0.8, reverb:false, distort:false, robot:false, bandQ:1},
  {name:'mwanamke', pitch:1.05, formant:0.6, reverb:false, distort:false, robot:false, bandQ:1},
  {name:'mlevi', pitch:0.85, formant:-1.2, reverb:true, distort:true, robot:false, bandQ:2},
  {name:'teja', pitch:1.1, formant:0.8, reverb:false, distort:false, robot:false, bandQ:1},
  {name:'old_mzee', pitch:0.7, formant:-2.5, reverb:true, distort:false, robot:false, bandQ:4},
  {name:'cartoon_high', pitch:1.8, formant:3.0, reverb:false, distort:true, robot:false, bandQ:1},
  {name:'robot', pitch:1.0, formant:-2.0, reverb:false, distort:true, robot:true, robotFreq:40}
];

// populate UI
function buildPresetUI(){
  presetListDiv.innerHTML = '';
  presets.forEach((p, i)=>{
    const b = document.createElement('button'); b.className='presetBtn'; b.textContent = p.name;
    b.onclick = ()=> { selectPresetIndex(i); };
    presetListDiv.appendChild(b);
  });
}
let selectedPresetIndex = 0;
function selectPresetIndex(i){
  selectedPresetIndex = i;
  const p = presets[i];
  pitch.value = p.pitch; pitch.oninput();
  formant.value = p.formant; formant.oninput();
  fxReverb.checked = !!p.reverb;
  fxDist.checked = !!p.distort;
  fxRobot.checked = !!p.robot;
  log('Selected preset ' + p.name);
}
buildPresetUI();
selectPresetIndex(0);

applyPresetBtn.onclick = ()=> {
  // build preset object from UI
  const preset = {
    name: 'custom',
    pitch: parseFloat(pitch.value),
    formant: parseFloat(formant.value),
    reverb: fxReverb.checked,
    distort: fxDist.checked,
    robot: fxRobot.checked,
    bandQ: document.getElementById('bandpass')?.checked ? 8 : 1
  };
  if(!rawAudioBuffer && !uploadFileBuffer){ alert('Hakuna sauti: rekodi au upload kwanza'); return; }
  const buf = rawAudioBuffer || uploadedAudioBuffer;
  playWithPreset(buf, preset);
};

storePresetBtn.onclick = ()=> {
  const p = {name: 'user_'+Date.now(), pitch:parseFloat(pitch.value), formant:parseFloat(formant.value), reverb:fxReverb.checked, distort:fxDist.checked, robot:fxRobot.checked, bandQ:1};
  presets.push(p);
  buildPresetUI();
  log('Custom preset saved: ' + p.name);
};

// ---------- upload handling ----------
let uploadedAudioBuffer = null;
upload.onchange = async (e) => {
  const f = e.target.files[0];
  if(!f) return;
  const arr = await f.arrayBuffer();
  const ctx = ensureAudioContext();
  try {
    uploadedAudioBuffer = await ctx.decodeAudioData(arr.slice(0));
    log('Uploaded audio decoded. Duration: ' + uploadedAudioBuffer.duration.toFixed(2) + 's');
    useUpload.disabled = false;
    playRaw.disabled = false; playFx.disabled = false; saveFx.disabled = false; speakWithPresetBtn.disabled=false;
    drawWave(uploadedAudioBuffer);
  } catch(e){ alert('Upload decode error: ' + e.message); log('Decode error: ' + e.message); }
};
useUpload.onclick = ()=> { rawAudioBuffer = uploadedAudioBuffer; log('Using uploaded audio as current buffer'); };

// ---------- stop play ----------
stopPlay.onclick = ()=> { stopCurrentPlayback(); log('Playback stopped by user'); };

// ---------- save processed WAV (simple resampling playbackRecord) ----------
saveFx.onclick = async ()=>{
  // approach: render the processed audio by playing it through OfflineAudioContext and capturing output
  if(!rawAudioBuffer){ alert('Hakuna sauti'); return; }
  const ctx = ensureAudioContext();
  const preset = { pitch: parseFloat(pitch.value), formant: parseFloat(formant.value), reverb: fxReverb.checked, distort: fxDist.checked, robot: fxRobot.checked };
  // offline length scaled by pitch
  const length = Math.ceil(rawAudioBuffer.length / (preset.pitch || 1.0));
  const offline = new OfflineAudioContext(rawAudioBuffer.numberOfChannels, length, ctx.sampleRate);
  const src = offline.createBufferSource(); src.buffer = rawAudioBuffer;
  src.playbackRate.value = preset.pitch || 1.0;

  const low = offline.createBiquadFilter(); low.type='lowshelf'; low.frequency.value=300; low.gain.value = (preset.formant||0)*-2;
  const high = offline.createBiquadFilter(); high.type='highshelf'; high.frequency.value=3000; high.gain.value=(preset.formant||0)*2;
  const band = offline.createBiquadFilter(); band.type='bandpass'; band.frequency.value=800; band.Q.value=1;

  src.connect(low); low.connect(high); high.connect(band);

  let last = band;
  if(preset.distort){ const sh = makeDistortion(offline, 400); last.connect(sh); last = sh; }
  if(preset.reverb){ const conv = offline.createConvolver(); conv.buffer = makeImpulse(offline, 1.2, 2.0); last.connect(conv); last = conv; }

  last.connect(offline.destination);
  src.start(0);
  log('Rendering processed audio (please wait)...');
  const rendered = await offline.startRendering();
  // convert to wav and download
  function bufferToWavABuffer(abuf){
    const numChannels = abuf.numberOfChannels;
    const length = abuf.length * numChannels;
    const buffer = new ArrayBuffer(44 + length * 2);
    const view = new DataView(buffer);
    function writeString(view, offset, string) {
      for (let i = 0; i < string.length; i++) view.setUint8(offset + i, string.charCodeAt(i));
    }
    writeString(view, 0, 'RIFF');
    view.setUint32(4, 36 + length * 2, true);
    writeString(view, 8, 'WAVE');
    writeString(view, 12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, numChannels, true);
    view.setUint32(24, abuf.sampleRate, true);
    view.setUint32(28, abuf.sampleRate * numChannels * 2, true);
    view.setUint16(32, numChannels * 2, true);
    view.setUint16(34, 16, true);
    writeString(view, 36, 'data');
    view.setUint32(40, length * 2, true);
    // write PCM
    let offset = 44;
    const channels = [];
    for (let i = 0; i < numChannels; i++) channels.push(abuf.getChannelData(i));
    const sampleCount = abuf.length;
    for (let i = 0; i < sampleCount; i++) {
      for (let ch = 0; ch < numChannels; ch++) {
        let s = Math.max(-1, Math.min(1, channels[ch][i]));
        view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
        offset += 2;
      }
    }
    return new Blob([view], {type:'audio/wav'});
  }
  const wavBlob = bufferToWavABuffer(rendered);
  const a = document.createElement('a'); a.href = URL.createObjectURL(wavBlob); a.download = 'kavishe-processed-'+Date.now()+'.wav'; a.click();
  URL.revokeObjectURL(a.href);
  log('Processed WAV downloaded.');
};

// ---------- Text -> Speech ----------
speakBtn.onclick = async ()=>{
  const text = ttsText.value.trim();
  if(!text) return;
  const utter = new SpeechSynthesisUtterance(text);
  utter.lang = ttsLang.value || 'sw-TZ';
  utter.rate = parseFloat(ttsRate.value || 1.0);
  // choose voice closest to lang
  const voices = speechSynthesis.getVoices();
  const v = voices.find(x=>x.lang && x.lang.startsWith(utter.lang.split('-')[0])) || voices[0];
  if(v) utter.voice = v;
  speechSynthesis.cancel(); // reset
  speechSynthesis.speak(utter);
  log('TTS speaking using ' + (utter.lang || 'default'));
};

// speak + apply preset: attempt to capture TTS into AudioContext then play with FX (browser support varies)
speakWithPresetBtn.onclick = async ()=> {
  const text = ttsText.value.trim();
  if(!text) return;
  // Approach: create an offscreen audio element that speaks via SpeechSynthesis and route to MediaElementSource -> WebAudio -> FX
  // NOTE: Many browsers won't allow capturing speechSynthesis output to <audio> src; so we attempt a fallback: speak normally and ask user to record manually if needed.
  try {
    const utter = new SpeechSynthesisUtterance(text);
    utter.lang = ttsLang.value || 'sw-TZ';
    utter.rate = parseFloat(ttsRate.value || 1.0);
    speechSynthesis.speak(utter);
    log('TTS spoken (FX capture limited). If you need FX applied to TTS, consider recording TTS playback or we can synthesize audio server-side.');
  } catch(e){ log('TTS error: ' + e.message); alert('TTS + FX is limited in browser. See log.'); }
};

// ---------- UI updates ----------
pitch.oninput = ()=> { pitchVal.textContent = parseFloat(pitch.value).toFixed(2) + 'x'; };
formant.oninput = ()=> { formVal.textContent = parseFloat(formant.value).toFixed(2); };
ttsRate.oninput = ()=> { ttsRateVal.textContent = parseFloat(ttsRate.value).toFixed(2); };

// ---------- initial voice list warming (some browsers need user gesture) ----------
window.addEventListener('click', ()=> { speechSynthesis.getVoices(); }, {once:true});

// Final note
log('Improved Voice Lab loaded. Record or upload a file, choose a preset and Play Preset. Use TTS for typed text.');
</script>
</body>
</html>